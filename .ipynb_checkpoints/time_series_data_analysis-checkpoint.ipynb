{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9y8L5Ie6oTvb"
   },
   "source": [
    "## **Previsão de acidentes em Recife**\n",
    "\n",
    "### Objetivo:\n",
    "Baseado nos registros de ocorrência de acidentes em Recife (2015-2019), criar um modelo de previsão de séries temporais. Para isso, os dados de 2015 à 2018 serão utilizados como treinamento do modelo. Já os dados de 2019 serão utilizados para validação do modelo aplicando-se a validação walk-foward.  \n",
    "\n",
    "### Etapas nesse notebook:\n",
    "1. Entendimento dos dados e criação do dataset\n",
    "2. Pré-processamento dos dados\n",
    "3. Identificação de outliers\n",
    "4. Análise dos dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CYWkWjTq4907"
   },
   "source": [
    "### 1. Entendimento dos dados e Criação do Dataset\n",
    "#### Planilhas\n",
    "**acidentes-transito-2015.csv**: 7273 x 24\n",
    "\n",
    "**acidentes_2016.csv**: 11263 x 47\n",
    "\n",
    "**acidentes_2017.csv**: 11758 x 32\n",
    "\n",
    "**acidentes_2018.csv**: 11411 x 45\n",
    "\n",
    "**acidentes-2019.csv**: 12062 x 47\n",
    "\n",
    "Todas foram concatenadas em um único DataFrame de dimensões 53767 x 24\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "1fqDLrNbqIPP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acidentes-2019.csv has shape of(12062, 47)\n",
      "acidentes-transito-2015.csv has shape of(7273, 24)\n",
      "acidentes_2016.csv has shape of(11263, 47)\n",
      "acidentes_2017.csv has shape of(11758, 32)\n",
      "acidentes_2018.csv has shape of(11411, 45)\n"
     ]
    }
   ],
   "source": [
    "# List every file in our dataset\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def get_filesname(PATH):\n",
    "  filesnames = os.listdir(PATH)\n",
    "  filesnames_csv = [f for f in filesnames if (f.lower().endswith(\".csv\"))]\n",
    "  return filesnames_csv\n",
    "\n",
    "def get_files_info(filesnames, PATH):\n",
    "  total_data = 0\n",
    "  for f in filesnames:\n",
    "    data = pd.read_csv(PATH + '/' + f)\n",
    "    print(str(f) + ' has shape of' + str(data.shape))\n",
    "    total_data += data.shape[0]\n",
    "  return total_data\n",
    "\n",
    "PATH = 'dataset'\n",
    "filesnames = get_filesname(PATH)\n",
    "total_data = get_files_info(filesnames, PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "XIXPiNcZqdo4"
   },
   "outputs": [],
   "source": [
    "# Create unique dataset\n",
    "def create_data_set(files):\n",
    "  main_df = pd.DataFrame(pd.read_csv(PATH + '/' + files[0]))\n",
    "  # Filter unname columns\n",
    "  main_df.drop(main_df.filter(regex=\"Unname\"),axis=1, inplace=True)\n",
    "  for i in range(1,len(files)):\n",
    "    data = pd.read_csv(PATH + '/' + files[i])\n",
    "    df = pd.DataFrame(data)\n",
    "    df.drop(df.filter(regex=\"Unname\"),axis=1, inplace=True)\n",
    "    main_df = pd.concat([main_df,df],axis=0, ignore_index=True)\n",
    "\n",
    "  return main_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "WIKf5xIduUme"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 53767 entries, 0 to 53766\n",
      "Data columns (total 42 columns):\n",
      " #   Column                     Non-Null Count  Dtype  \n",
      "---  ------                     --------------  -----  \n",
      " 0   data                       53767 non-null  object \n",
      " 1   hora                       53712 non-null  object \n",
      " 2   natureza_acidente          53347 non-null  object \n",
      " 3   situacao                   53715 non-null  object \n",
      " 4   bairro                     53475 non-null  object \n",
      " 5   endereco                   53544 non-null  object \n",
      " 6   numero                     23998 non-null  object \n",
      " 7   detalhe_endereco_acidente  6656 non-null   object \n",
      " 8   complemento                50597 non-null  object \n",
      " 9   endereco_cruzamento        46360 non-null  object \n",
      " 10  numero_cruzamento          34215 non-null  object \n",
      " 11  referencia_cruzamento      42189 non-null  object \n",
      " 12  bairro_cruzamento          46108 non-null  object \n",
      " 13  num_semaforo               5745 non-null   object \n",
      " 14  sentido_via                11871 non-null  object \n",
      " 15  tipo                       44643 non-null  object \n",
      " 16  descricao                  39506 non-null  object \n",
      " 17  auto                       49504 non-null  object \n",
      " 18  moto                       18689 non-null  object \n",
      " 19  ciclom                     6699 non-null   object \n",
      " 20  ciclista                   2932 non-null   object \n",
      " 21  pedestre                   1831 non-null   object \n",
      " 22  onibus                     7220 non-null   object \n",
      " 23  caminhao                   5078 non-null   object \n",
      " 24  viatura                    1582 non-null   object \n",
      " 25  outros                     1151 non-null   object \n",
      " 26  vitimas                    45625 non-null  float64\n",
      " 27  vitimasfatais              1088 non-null   float64\n",
      " 28  acidente_verificado        12263 non-null  object \n",
      " 29  tempo_clima                11342 non-null  object \n",
      " 30  situacao_semaforo          11601 non-null  object \n",
      " 31  sinalizacao                11362 non-null  object \n",
      " 32  condicao_via               11553 non-null  object \n",
      " 33  conservacao_via            11375 non-null  object \n",
      " 34  ponto_controle             10450 non-null  object \n",
      " 35  situacao_placa             10467 non-null  object \n",
      " 36  velocidade_max_via         3815 non-null   object \n",
      " 37  mao_direcao                11109 non-null  object \n",
      " 38  divisao_via1               10698 non-null  object \n",
      " 39  divisao_via2               1550 non-null   object \n",
      " 40  divisao_via3               534 non-null    object \n",
      " 41  natureza                   7206 non-null   object \n",
      "dtypes: float64(2), object(40)\n",
      "memory usage: 17.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df = create_data_set(filesnames)\n",
    "df.head()\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "pm786sVN9Cgm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct Dataset\n"
     ]
    }
   ],
   "source": [
    "# Check dataset\n",
    "if total_data == df.shape[0]:\n",
    "  print('Correct Dataset')\n",
    "else:\n",
    "  print('Wrong Dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PGTOSu_Hyxb6"
   },
   "source": [
    "A ingestão dos dados foi bem sucedida, pois não se perdeu nenhum registro na integração dos datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ISUHtcf095s-"
   },
   "source": [
    "### 2. Pré-processamento dos dados\n",
    "\n",
    "Por serem dados históricos, é esperado que os dados estejam com \"status\" de **FINALIZADO**. Registros com situação diferente podem ser tratados como erros na ingestão dos dados.\n",
    "\n",
    "Além desse campo, foram analisados os tipos de acidentes possíveis, sendo encontradas 3252 ocorrências distintas. Observando mais atentamente os dados, percebe-se que há algum tipo de falha em seu posicionamento na tabela, atribuindo endereços aos tipos de acidentes.\n",
    "\n",
    "Contudo, para o que está com status de FINALIZADO, foi decidido manter tais informações, pois houve um consentimento da pessoa/sistema que aprovou o registro *(esse é um caso a ser discutido com o time de negócio)*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "AqSvJXWY91cb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['data', 'hora', 'natureza_acidente', 'situacao', 'bairro', 'endereco',\n",
       "       'numero', 'detalhe_endereco_acidente', 'complemento',\n",
       "       'endereco_cruzamento', 'numero_cruzamento', 'referencia_cruzamento',\n",
       "       'bairro_cruzamento', 'num_semaforo', 'sentido_via', 'tipo', 'descricao',\n",
       "       'auto', 'moto', 'ciclom', 'ciclista', 'pedestre', 'onibus', 'caminhao',\n",
       "       'viatura', 'outros', 'vitimas', 'vitimasfatais', 'acidente_verificado',\n",
       "       'tempo_clima', 'situacao_semaforo', 'sinalizacao', 'condicao_via',\n",
       "       'conservacao_via', 'ponto_controle', 'situacao_placa',\n",
       "       'velocidade_max_via', 'mao_direcao', 'divisao_via1', 'divisao_via2',\n",
       "       'divisao_via3', 'natureza'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "GjnajBen_Z8J"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FINALIZADA                42521\n",
       "CANCELADA                  9888\n",
       "EM ATENDIMENTO              575\n",
       "PENDENTE                    422\n",
       "DUPLICIDADE                 150\n",
       "EQUIPE EM DESLOCAMENTO      113\n",
       "EM ABERTO                    33\n",
       "EQUIPE NO LOCAL              13\n",
       "Name: situacao, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['situacao'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oJDOyBmV-hCD"
   },
   "outputs": [
    {
     "ename": "ConversionError",
     "evalue": "Failed to convert value(s) to axis units: 0        FINALIZADA\n1        FINALIZADA\n2         CANCELADA\n3         CANCELADA\n4        FINALIZADA\n            ...    \n53762    FINALIZADA\n53763    FINALIZADA\n53764    FINALIZADA\n53765    FINALIZADA\n53766     CANCELADA\nName: x, Length: 53767, dtype: object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\matplotlib\\axis.py\u001b[0m in \u001b[0;36mconvert_units\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m   1519\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1520\u001b[1;33m             \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconverter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1521\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\matplotlib\\category.py\u001b[0m in \u001b[0;36mconvert\u001b[1;34m(value, unit, axis)\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[1;31m# force an update so it also does type checking\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m         \u001b[0munit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvectorize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mapping\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0motypes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\matplotlib\\category.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    210\u001b[0m             \u001b[1;31m# OrderedDict just iterates over unique values in data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m             \u001b[0mcbook\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_isinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    212\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mconvertible\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\matplotlib\\cbook\\__init__.py\u001b[0m in \u001b[0;36m_check_isinstance\u001b[1;34m(_types, **kwargs)\u001b[0m\n\u001b[0;32m   2245\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2246\u001b[1;33m             raise TypeError(\n\u001b[0m\u001b[0;32m   2247\u001b[0m                 \"{!r} must be an instance of {}, not a {}\".format(\n",
      "\u001b[1;31mTypeError\u001b[0m: 'value' must be an instance of str or bytes, not a float",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mConversionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-f19d4ca60a52>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfig_dims\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msharey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0msns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"situacao\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\seaborn\\distributions.py\u001b[0m in \u001b[0;36mhistplot\u001b[1;34m(data, x, y, hue, weights, stat, bins, binwidth, binrange, discrete, cumulative, common_bins, common_norm, multiple, element, fill, shrink, kde, kde_kws, line_kws, thresh, pthresh, pmax, cbar, cbar_ax, cbar_kws, palette, hue_order, hue_norm, color, log_scale, legend, ax, **kwargs)\u001b[0m\n\u001b[0;32m   1420\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munivariate\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1421\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1422\u001b[1;33m         p.plot_univariate_histogram(\n\u001b[0m\u001b[0;32m   1423\u001b[0m             \u001b[0mmultiple\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmultiple\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1424\u001b[0m             \u001b[0melement\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0melement\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\seaborn\\distributions.py\u001b[0m in \u001b[0;36mplot_univariate_histogram\u001b[1;34m(self, multiple, element, fill, common_norm, common_bins, shrink, kde, kde_kws, color, legend, line_kws, estimate_kws, **plot_kws)\u001b[0m\n\u001b[0;32m    421\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    422\u001b[0m         \u001b[1;31m# First pass through the data to compute the histograms\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 423\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0msub_vars\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msub_data\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miter_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"hue\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfrom_comp_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    424\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    425\u001b[0m             \u001b[1;31m# Prepare the relevant data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\seaborn\\_core.py\u001b[0m in \u001b[0;36miter_data\u001b[1;34m(self, grouping_vars, reverse, from_comp_data)\u001b[0m\n\u001b[0;32m    965\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    966\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfrom_comp_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 967\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcomp_data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    968\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    969\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot_data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\seaborn\\_core.py\u001b[0m in \u001b[0;36mcomp_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1034\u001b[0m                 \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mf\"{var}axis\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1035\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1036\u001b[1;33m                 \u001b[0mcomp_var\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_units\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1037\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_scale\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"log\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                     \u001b[0mcomp_var\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog10\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcomp_var\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\matplotlib\\axis.py\u001b[0m in \u001b[0;36mconvert_units\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m   1520\u001b[0m             \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconverter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1521\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1522\u001b[1;33m             raise munits.ConversionError('Failed to convert value(s) to axis '\n\u001b[0m\u001b[0;32m   1523\u001b[0m                                          f'units: {x!r}') from e\n\u001b[0;32m   1524\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mConversionError\u001b[0m: Failed to convert value(s) to axis units: 0        FINALIZADA\n1        FINALIZADA\n2         CANCELADA\n3         CANCELADA\n4        FINALIZADA\n            ...    \n53762    FINALIZADA\n53763    FINALIZADA\n53764    FINALIZADA\n53765    FINALIZADA\n53766     CANCELADA\nName: x, Length: 53767, dtype: object"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "fig_dims = (25, 8)\n",
    "fig, ax = plt.subplots(1,1, figsize=fig_dims, sharey=True)\n",
    "\n",
    "sns.histplot(x=\"situacao\", ax=ax, data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "njFQ3Fh-AUtp"
   },
   "outputs": [],
   "source": [
    "df['tipo'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S9tDk2kQAyxS"
   },
   "outputs": [],
   "source": [
    "filter_tipo = df.groupby('tipo')['tipo'].filter(lambda x: len(x) >= 10)\n",
    "fig_dims = (35, 10)\n",
    "fig, ax = plt.subplots(1,1, figsize=fig_dims, sharey=True)\n",
    "\n",
    "sns.histplot(x=filter_tipo, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kAnJwjKTBkxp"
   },
   "outputs": [],
   "source": [
    "# Dataset has some shifted feature values\n",
    "shifted_df = df[(df['tipo'] != 'COLISÃO') & (df['situacao'] == 'FINALIZADA')]\n",
    "shifted_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "76ex3fL6Qxyr"
   },
   "outputs": [],
   "source": [
    "def filter_data(df):\n",
    "  filter_df = df[df['situacao'] == 'FINALIZADA']\n",
    "  #TODO: include more filters?\n",
    "  return filter_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SQVR1Z2RDoUi"
   },
   "outputs": [],
   "source": [
    "filter_df = filter_data(df)\n",
    "filter_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ucHP4zuzB2W"
   },
   "source": [
    "Este é um dos tópicos que precisam ser discutidos com o time de negócios. Fiz uma inferência de que os dados com situação diferente de \"FINALIZADO\" não são importantes para a série temporal. Isso pode ser revisado de acordo com as regras de negócio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7NB7Lhj8EKoD"
   },
   "source": [
    "### 3. Identificação de outliers\n",
    "\n",
    "Foram implementadas duas abordagens para identificação de outliers:\n",
    "\n",
    "**DBSCan:** método não supervisionado que busca gerar clusters a partir da população proposta. Como o dataset possui muitos dados categóricos, foram selecionadas duas características para serem analisadas (tipo e natureza_acidente).  De acordo com os parâmetros de entrada, um cluster é formado por no mínimo 40 pontos (min_sample). Aqueles que ficarem distantes dos clusters formados (eps) são considerados *outliers*. \n",
    "\n",
    "**Heurística:** método customizado que considera a frequência dos tipos de acidentes. A heurística seleciona apenas tipos com mais do que 10 ocorrências e que não contenham a \"palavra\" **SENT**. Os registros que não se enquadram nessas regras são tidos como *outliers*. *(Esse tipo de heurística precisa ser analisada e validada pelo time de negócios)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vzeHbiPhRbFO"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def get_data_outliers(df):\n",
    "  # Change to category columns\n",
    "  df[\"tipo\"] = df[\"tipo\"].astype('category')\n",
    "  df[\"natureza_acidente\"] = df[\"natureza_acidente\"].astype('category')\n",
    "  df['tipo_num'] = df[\"tipo\"].cat.codes\n",
    "  df['natureza_acidente_num'] = df[\"natureza_acidente\"].cat.codes\n",
    "\n",
    "  outliers_df = df[['tipo_num', 'natureza_acidente_num']]\n",
    "  outliers_df = StandardScaler().fit_transform(outliers_df)\n",
    "  db = DBSCAN(eps=0.3, min_samples = 40).fit(outliers_df)\n",
    "  labels = db.labels_\n",
    "  print(len(labels))\n",
    "  df['outliers'] = labels\n",
    "  results = df[df['outliers'] != -1]\n",
    "  \n",
    "  return results\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YPSwttCEV7Zx"
   },
   "outputs": [],
   "source": [
    "def get_data_custom_outliers(df):\n",
    "  df_type = df.groupby('tipo').size().reset_index()\n",
    "  df_type = df_type.rename(columns={0: \"Freq\"})\n",
    "  df_type_filter = df_type[(df_type['Freq'] > 10)]\n",
    "\n",
    "  df_outliers = df_type_filter[~df_type_filter[\"tipo\"].str.contains('SENT')]\n",
    "  correct_types = list(df_outliers['tipo'])\n",
    "\n",
    "  bool_type = df.tipo.isin(correct_types)\n",
    "  print(len(bool_type))\n",
    "  results = df[bool_type]\n",
    "  return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ccrk1X25YLXJ"
   },
   "outputs": [],
   "source": [
    "def pre_processing(type, df):\n",
    "  filter_df = filter_data(df)\n",
    "  if type=='custom':\n",
    "    results = get_data_custom_outliers(filter_df)\n",
    "  elif type == 'dbscan':\n",
    "    results = get_data_outliers(filter_df)\n",
    "  else:\n",
    "    result = filter_df\n",
    "  return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DPvyA82vY6z5"
   },
   "outputs": [],
   "source": [
    "filter_df = pre_processing(\"dbscan\", df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-91vutfJzYZ5"
   },
   "source": [
    "Aqui é mais um tópico que precisa ser alinhado com o time de negócio. Decidi manter o \"DBScan\" como filtro final, pois ele identificou poucos outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gozD2M6jKkJo"
   },
   "source": [
    "### 4. Análise dos dados\n",
    "\n",
    "Análise dos dados agrupados ao longo do tempo (série temporal).\n",
    "\n",
    "Etapas\n",
    "1. Análise preliminares\n",
    "2. Análise dos lags\n",
    "3. Análise de estacionariedade\n",
    "4. Análise de sazonalidade\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H8PMKOb9KzWx"
   },
   "source": [
    "#### 4.1 Análises preliminares\n",
    "Ao fazermos o agrupamento dos dados, é necessário garantir que todos os períodos que estejam dentro do range requerido existem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7AVrZCy4KqU3"
   },
   "outputs": [],
   "source": [
    "# Compare date_range values\n",
    "def check_date_range(df):\n",
    "  df['data'] = pd.to_datetime(df['data'])\n",
    "  nominal_days = ((max(df.data) - min(df.data)).days+1)\n",
    "  actual_days = len(df['data'].value_counts())\n",
    "  return nominal_days == actual_days\n",
    "\n",
    "# Fix date_range if necessary\n",
    "def fix_date_range(df):\n",
    "  print('df length: ')\n",
    "  print(len(df))\n",
    "  idx = pd.date_range(min(df['data']), max(df['data']))\n",
    "  df_i = df.set_index('data')\n",
    "  df_i.index = pd.DatetimeIndex(df_i.index)\n",
    "  df_i = df_i.reindex(idx, fill_value=0)\n",
    "  print('new_df length: ')\n",
    "  print(len(df_i))\n",
    "  return df_i\n",
    "\n",
    "# Group data with frequency\n",
    "def group_data(df):\n",
    "  df_freq = df.groupby('data').size().reset_index()\n",
    "  df_freq = df_freq.rename(columns={0: \"Freq\"})\n",
    "  if check_date_range(df_freq):\n",
    "    print('Same date range length.')\n",
    "    results = df_freq.set_index('data')\n",
    "  else:\n",
    "    print('Different date range length.')\n",
    "    results = fix_date_range(df_freq)\n",
    "    \n",
    "  return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LxHP3WxXMSk4"
   },
   "outputs": [],
   "source": [
    "df_freq = group_data(filter_df)\n",
    "df_freq.to_csv('grouped_accident_data.csv')\n",
    "df_freq.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kmuhLs7zL8pi"
   },
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"darkgrid\")\n",
    "fig_dims = (35, 10)\n",
    "fig, ax = plt.subplots(1,1, figsize=fig_dims, sharey=True)\n",
    "\n",
    "sns.lineplot(x = df_freq.index, y=\"Freq\", data=df_freq, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u4nQPMCvM6Ij"
   },
   "outputs": [],
   "source": [
    "sample_df = df_freq['2018-01-01':'2018-06-30']\n",
    "\n",
    "sns.set_theme(style=\"darkgrid\")\n",
    "fig_dims = (35, 10)\n",
    "fig, ax = plt.subplots(1,1, figsize=fig_dims, sharey=True)\n",
    "\n",
    "sns.lineplot(x = sample_df.index, y=\"Freq\", data=sample_df, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nopexn9jX_nv"
   },
   "source": [
    "#### 4.2 Análise dos lags\n",
    "\n",
    "Para análise dos *lags* relevantes da série temporal, podemos utilizar duas técnicas complementares:\n",
    "1. **Naive model:** criação de um modelo considerando apenas o deslocamento dos valores na série temporal. Métrica de análise: RMSE\n",
    "2. **Gráficos de autocorrelação e autocorrelação parcial:** análise da autocorrelação dos lags considerando os dados passados e os resíduos presentes na série. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-R4ZMBKfigMq"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def check_lags(df_freq, num_lags):\n",
    "  results = []\n",
    "  for i in range(num_lags):\n",
    "    new_sample = {}\n",
    "    shift_df = df_freq.shift(i+1, fill_value=0)\n",
    "    new_sample['i'] = i+1\n",
    "    new_sample['rmse'] = mean_squared_error(df_freq['Freq'], shift_df['Freq'], squared=False)\n",
    "    results.append(new_sample)\n",
    "  return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nFJhx66ujgcQ"
   },
   "outputs": [],
   "source": [
    "results = check_lags(df_freq, 50)\n",
    "sns.lineplot(data=results, x=\"i\", y=\"rmse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C1kbrNpqVc-U"
   },
   "outputs": [],
   "source": [
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n",
    "plot_acf(df_freq['Freq'], lags=30)\n",
    "plot_pacf(df_freq['Freq'], lags=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r4877GFr_h5e"
   },
   "source": [
    "Pelo *naive model* é possível perceber que o **lag 7** possui grande correlação na série. Isso também é confirmado pelos gráficos de correlação. Além disso, é possível perceber que o **primeiro lag** também possui uma correlação considerável com os dados que estão sendo estudados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mk365WzOtNb-"
   },
   "source": [
    "#### 4.3 Análise de estacionariedade\n",
    "Uma série é tido como estacionária quando o seu comportamento ao longo do tempo não possui uma tendência. Para verificar se a série é estacionária ou não, existe uma quantidade relevante de abordagens. Para esse caso, decidimos utilizar algumas:\n",
    "\n",
    "1. **Distribuição dos valores da série:** verificar se os valores se aproximam de uma distribuição normal. Em caso afirmativo, existe uma forte premissa que a séria é estacionária\n",
    "2. **Decomposição da série:** análise do comportamento da tendência para a série decomposta.\n",
    "3. **Teste de hipótese Dickey-Fuller:** teste em que a hipótese nula afirma a presença de raíz unitária na série. Em caso de rejeição da hipótese nula, a série é tida como estacionária pois refuta a raíz unitária."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pmZU2pnfyrNu"
   },
   "outputs": [],
   "source": [
    "fig_dims = (15, 8)\n",
    "fig, ax = plt.subplots(1,1, figsize=fig_dims, sharey=True)\n",
    "sns.histplot(data=df_freq, x=\"Freq\", ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7Lx_7QNgzh7P"
   },
   "outputs": [],
   "source": [
    "from statsmodels.graphics.gofplots import qqplot\n",
    "fig_dims = (15, 8)\n",
    "fig, ax = plt.subplots(1,1, figsize=fig_dims, sharey=True)\n",
    "qqplot(df_freq['Freq'], line='s', ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rw0AVm6emBw0"
   },
   "outputs": [],
   "source": [
    "df_freq['ano'] = pd.DatetimeIndex(df_freq.index).year\n",
    "\n",
    "sns.boxplot(x=\"ano\", y=\"Freq\",\n",
    "            #hue=\"weekend\", palette=[\"m\", \"g\"],\n",
    "            data=df_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fG1NQdviGOFc"
   },
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "full_ts_decomp = seasonal_decompose(df_freq['Freq'], freq=365)\n",
    "fig, (ax1,ax2,ax3,ax4) = plt.subplots(4,1, figsize=(23,12))\n",
    "full_ts_decomp.observed.plot(ax=ax1)\n",
    "full_ts_decomp.trend.plot(ax=ax2)\n",
    "full_ts_decomp.seasonal.plot(ax=ax3)\n",
    "full_ts_decomp.resid.plot(ax=ax4)\n",
    "\n",
    "decomposition.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZVA9JlITzvvw"
   },
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "dftest = adfuller(df_freq['Freq'])\n",
    "print('ADF Statistic: %f' % dftest[0])\n",
    "print('p-value: %f' % dftest[1])\n",
    "print('Critical Values:')\n",
    "for key, value in dftest[4].items():\n",
    "\tprint('\\t%s: %.3f' % (key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xxS9Nphf_Bm1"
   },
   "source": [
    "O teste de Dickey-Fuller nos aponta que a série é tida como estacionária. Além disso, a distribuição dos valores da série contribui com essa teoria. Contudo, o gráfico de tendência da série decomposta nos faz refletir um pouco mais profundamente. A princípio é possível perceber uma tendência, mas analisando o comportamento de 2015, 2016 e 2018, é possível perceber uma estabilidade. Já em 2017 e 2019 percebemos uma tendência de alta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GC3-ea_7nMLd"
   },
   "source": [
    "#### 4.4 Análise de sazonalidade\n",
    "Para análise de sazonalidade, foram utilizadas 3 abordagens distintas:\n",
    "1. Comportamento dos registros ao longo dos dias da semana através de um gráfico boxplot\n",
    "2. Comportamento dos registros ao longo dos meses através de um gráfico boxplot\n",
    "3. Comportamento da sazonalidade da série decomposta considerando uma amostragem da série em questão."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hY3P3JdMnG0I"
   },
   "outputs": [],
   "source": [
    "df_freq['weekday'] = df_freq.index.weekday\n",
    "df_freq['weekend'] = (df_freq.index.weekday // 5 == 1).astype(int)\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(15,8))\n",
    "sns.boxplot(x=\"weekday\", y=\"Freq\",\n",
    "            hue=\"weekend\", palette=[\"m\", \"g\"],\n",
    "            data=df_freq, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IHiTbZLxlQZC"
   },
   "outputs": [],
   "source": [
    "df_freq['mes'] = pd.DatetimeIndex(df_freq.index).month\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(15,8))\n",
    "sns.boxplot(x=\"mes\", y=\"Freq\",\n",
    "            #hue=\"weekend\", palette=[\"m\", \"g\"],\n",
    "            data=df_freq, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rEr6oh3QhOGs"
   },
   "outputs": [],
   "source": [
    "sample_df = df_freq['2017-01-01':'2017-12-31']\n",
    "sample_ts_decomp = seasonal_decompose(sample_df['Freq'], freq=7)\n",
    "fig, (ax1,ax2,ax3,ax4) = plt.subplots(4,1, figsize=(23,12))\n",
    "sample_ts_decomp.observed.plot(ax=ax1)\n",
    "sample_ts_decomp.trend.plot(ax=ax2)\n",
    "sample_ts_decomp.seasonal.plot(ax=ax3)\n",
    "sample_ts_decomp.resid.plot(ax=ax4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mQ7Z3qzd-P0c"
   },
   "source": [
    "Pelos gráficos propostos, é possível perceber que existe uma sazonalidade no **lag 7**. Isso é bem coerente, pois estamos lidando com dados diários, ou seja, existe uma correlação entre os dias da semana"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "CYWkWjTq4907",
    "H8PMKOb9KzWx",
    "Nopexn9jX_nv",
    "mk365WzOtNb-",
    "GC3-ea_7nMLd"
   ],
   "name": "time_series_data_analysis.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
